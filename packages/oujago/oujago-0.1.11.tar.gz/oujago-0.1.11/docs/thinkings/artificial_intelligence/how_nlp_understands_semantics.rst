=============================
NLP如何让计算机理解语义
=============================

问题
====

诱发我思考语义这个问题的，是另外一个问题。就是，为什么深度学习（DL）在自然语言处理
（NLP）上的任务的效果不是很好，而在计算机视觉（CV）或者语音识别等任务上效果好得要命？
如果这篇博客 `Deep Learning in NLP（一）词向量和语言模型 <http://licstar.net/archives/328>`_
中所说：“语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和
图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。”是正确的，
那为什么深度学习不能理解语义？或者说，NLP如果让计算机理解语义？

区别
=====

要理解这CV和NLP之间的区别，我们首先要知道视觉和听觉信号与自然语言之间的区别。

认知变化
----------

上面说道，视觉与听觉信号属于一些“原始信号输入”，属于人的感性认识的范围。而自然语言属于人的
“高层认识抽象”，属于人的理性认识的范围。感性认知的事物，是静态的，不在空间与时间中变化，而
且，事物与事物之间的联系不甚强烈。而理性认知的事物，是动态的，单个事物存在变动，多个事物之间
存在联系。

深度学习其实并不擅长动态事物的认知，同一个事物，如果在时间或者空间中变动，深度学习无法或者很
难对其进行认知。首先，它很难认知到不断变化的都是同一事物，其次，它很难认知到同一事物的不断变
化，即认知到变化本身。而自然语言，其次大多时候都在描述事物之间的变化、演进、联系。如“小王打
篮球”，描述的是“小王”与“篮球”之间的“打”的关系；又如“”小王先在图书馆看书，后来又去操
场打篮球。”，描述的是“小王”在时间（“先”、“后”）以及空间（“图书馆”、“操场”）与事
物”书“、”篮球“之间的变动关系。

表示
------

另外，视觉等信号在计算机中有很好的表示。图像通过像素来组合堆积，而像素在计算机中用 :math:`0-256`
之间的数值进行组合就可以表示。这种表示是很明白、很自然的。然而，自然语言在计算机中却没有很
好的表示。我个人觉得，这才是现在为何计算机处理自然语言的任务表现出无力的真正原因。以前
的one hot就不用说了。现在流行的word embedding，其实并不能真正让计算机表示自然语言想表达的
东西。当然，首先得肯定，word embedding的确比one hot更有效得多。

* 首先，对于静态的物体，word embedding能有效表示。它觉得，我们可以将一个物体，如“猫”、
  “狗”，用空间中的N维实物向量来表示，如果物体A与物体B之间相似程度较高，那么，A和B的N维
  向量距离也更近。这相较于one hot是一个大进步。
* 但是，就算是表示物体、表示事物，word embedding也会无力。它对于刚才我们举出的这种表层
  的事物与浅显的关系能有效表示，但是，如果稍微加入一点复杂的关系与抽象的概念描述，它就表现
  出无力感了！如对于层次关系。“猫”和“狗”都属于“动物”，“花”和“草”属于“植物”。
  word embedding能有效地说，“猫”与“狗”之间的距离比与“花”或“草”之间的距离近，但是，
  能有效地说“猫”与“动物”的距离比与“植物”的距离近吗？
* 甚至，对于“猫”属于“动物”、“草”属于“植物”，这种“属于”关系，word embedding能描述吗？
* 还有，对于否定词“不”、“非”、“not”等词，用N维实数向量表示出来，是什么意思？是说，
  这种否定词，包括其他的数词、量词、形容词，与实词一样都是同一个N维空间的存在？实词与实词之
  间的相似关系N维空间中的向量能描述，那么形容词修饰实词这种修饰关系如何描述出来？

解决
=====

所以，综合上述，归纳如下：NLP等技术为何难以描述语义，是因为这些方法无法描述动态、变化、
关系。而无法描述动态、变化、关系，是因为我们没有很好的表示。

这个世界万事万物都是在动态变化的，NLP将每一个事物、以及事物的每一个状态、事物的每一个
变化都静态化地描述成一个一个的词，这些词对应N维空间中的一个向量。它将**事物的变化**、
**事物之间的关系**与**事物**用同一种方式来表达。这是它的弱点。

所以，我个人觉得的一个突破点就是，如何区别描述这些表示事物、事物变化、事物关系的**词**，
即不同类别的词用不同的方式来表示。但其实，我们可以发现，事物、事物变化，与事物关系，
其实在自然语言中，都是用一种方式来表示的，那就是词！自然语言将不同维度、不同种类、不
同性质的东西用同一种方式进行描述，那说明，这种描述方式本身是比被描述对象高一级的东西，
是更抽象的东西。那NLP如果要理解这种更抽象的描述，唯一的方法，就是\textbf{还原}回去！
还原到具体的、有区别的被描述对象！

所以，如果要让NLP理解语义，我们不应该让它用概率的方式去猜测、统计一个一个的词，即，
不应该让它去**学词**，而是应该让它去学**词背后表示的事物**。

问题的问题
==============

最后，我突然发现，其实我们的问题并没有明确。就是，什么叫语义？什么又叫理解了语义？
我实在不知道如何回答。就像中文屋实验，计算机就是能够用某种方法让自己虽然不懂中文，
但就是能够自由交流。那我们需要让它理解语义吗？

``2016-11-09`` -- ``2016-11-12``

